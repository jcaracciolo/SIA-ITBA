source('Utils')
source('NeuralFunctionsBatch')
% inputSize: Size of each input
% outputSize: Size of the output
% layersSize: Size of each layer
% learningRate: Rate of learning
source('config')

% Trainer functions
% trainerPath: Path to trainer file
%  -- ? getSampleSize()
%  -- ? getSample(int index)
%  -- ? getAnswer(int index)
source(trainerPath)
setUpBatch()

% %  Activation functions
% % activationFunctionPath: Path to activate function
% %  -- float activate(float x)
% %  -- float correction(float x) derivative
% source(activationFunctionPath)

% % Create array with all sizes
% layersSize = [layersSize, outputSize];
% global layersCount = length(layersSize)

% % Setup RNG
% if (exist('randomSeed', 'var') == 1)
%     rand('seed',randomSeed)
% else
% 	seed = rand('seed')
% 	save randState seed
% 	rand('seed', seed)
% endif

% sampleSize = getSampleSize();
% samples = randperm(sampleSize);
% trainingSize = ceil(sampleSize*trainingPercentage);
% testingSize = sampleSize - trainingSize;
% trainingIndexes = samples(1:trainingSize);
% testingIndexes = samples(trainingSize+1:end);

% %create initial random matrix
% global weights = cell(layersCount,1);
% global deltaWeights = cell(layersCount,1);

% % Initial weights takes into account inputSize
% weights(1) = rand(layersSize(1), inputSize+1);
% deltaWeights(1) = zeros(layersSize(1), inputSize+1);

% for i=2:layersCount
% 	% Matrix size is layer(i+1) X (layer(i)+1)
% 	weights(i) = rand(layersSize(i), layersSize(i-1)+1);
% 	deltaWeights(i) = zeros(layersSize(i), layersSize(i-1)+1);
% endfor

% %Output vector for each layer
% global rawOutputs = cell(layersCount, trainingSize);
% global outputs = cell(layersCount, trainingSize);
% global deltas = cell(layersCount,trainingSize);

% global errors = [];
% global consecutiveErrorReduction=0;
% global epochs = 0;



% while(epochs<500)
% 	epochs +=1

% 	% Input must be a column vector with a -1 at the beggining
% 	input = getSampleMatrix();
% 	lastOutput = makeGuessWithMatrix(input);
% 	expected = getAnswerMatrix();

% 		%Correcting
% 		% Calculate the last delta to start backpropagation
% 		deltas{end, j} = times(correction(rawOutputs{end, j}), (expected - lastOutput));

% 		% For each delta, calculate it by correcting the output of the given layer
% 		% point product by the matrix product of
% 		% the weigths of the upper layer (removing the threshold column) and
% 		% the delta of the upper layer. 
% 		for i=fliplr(1:layersCount-1)
% 			deltas{i, j} = times(correction(rawOutputs{i, j}), (weights{i+1}(:,2:end)'*deltas{i+1, j}));
% 		endfor
% 	endfor

% 	adaptEta(trainingIndexes)

% 	for i = 1:layersCount
% 		deltaWeights{i} = zeros(size(weights{i}));
% 	endfor

% 	for j = 1 : layersCount
% 		deltaWeights{1} += deltas{1, j}*([-1; getSample(j)']');
% 		for(i=2:layersCount)
% 			deltaWeights{i} += deltas{i, j}*([-1; outputs{i-1, j}]');
% 		endfor
% 	endfor

% 	for i = 1:layersCount
% 		weights{i} += learningRate*deltaWeights{i};
% 	endfor
% endwhile

% correct = 0;
% for test = testingIndexes
% 	input = getSample(test)';
% 	% Batch
% 	guess = makeGuess(input, 1);
% 	expected = getAnswer(test);

% 	if((guess - expected)<validateEpsilon)
% 		correct+=1;
% 	endif
% endfor

% printf('Guessed %d out of %d. Ratio: %f\n',correct, testingSize,correct/testingSize)
% plot(1:length(errors),errors) 
% pause(1000)