source('Utils')
% inputSize: Size of each input
% outputSize: Size of the output
% layersSize: Size of each layer
% learningRate: Rate of learning
source('config')

% Trainer functions
% trainerPath: Path to trainer file
%  -- ? getSample(int index)
%  -- ? getAnswer(int index)
source(trainerPath)

%  Activation functions
% activationFunctionPath: Path to activate function
%  -- float activate(float x)
%  -- float correction(float x) derivative
source(activationFunctionPath)

% Create array with all sizes
layersSize = [layersSize, outputSize];
layersCount = length(layersSize)

%create initial random matrix
weights = cell(layersCount, 1);

% Initial weights takes into account inputSize
weights(1) = rand(layersSize(1), inputSize + 1);

for i=2:layersCount
	% Matrix size is layer(i+1) X (layer(i)+1)
	weights(i) = rand(layersSize(i), layersSize(i-1)+1);
endfor

%Output vector for each layer
rawOutputs = cell(layersCount,1);
outputs = cell(layersCount,1);

tries=0;
correct=0;

while(true)
	index = mod(randi(2**inputSize), 2**inputSize)+1;
	% Input must be a column vector with a -1 at the beggining
	input = getSample(index)';

	% Caculate the output of the first layer
	rawOutputs{1} = weights{1}*[-1; input];
	outputs{1} = activate(rawOutputs{1});

	% Caculate the output of each layer
	for i=2:layersCount
		% Wigths of the current layer times last output with a -1 at the beggining
		rawOutputs{i} = weights{i}*[-1; outputs{i-1}];
		outputs{i} = activate(rawOutputs{i});
	endfor

	%Correcting
	deltas = cell(layersCount,1);
	expected = getAnswer(index)';

	% Calculate the last delta to start backpropagation
	lastOutput = step(outputs{end});
	deltas{end} = times(correction(rawOutputs{end}), (expected - lastOutput));

	% For each delta, calculate it by correcting the output of the given layer
	% point product by the matrix product of
	% the weigths of the upper layer (removing the threshold column) and
	% the delta of the upper layer. 
	for i=fliplr(1:layersCount-1)
		deltas{i} = times(correction(rawOutputs{i}), (weights{i+1}(:,2:end)'*deltas{i+1}));
	endfor


	weights{1} += learningRate*(deltas{1}*([-1; input]'));
	for(i=2:layersCount)
		weights{i} += learningRate*(deltas{i}*([-1; outputs{i-1}]'));
	endfor

	tries += 1;
	guess = step(outputs{end});
	expected;
	if(guess == expected) 
		correct += 1;
	endif
	tries;
	ratio = correct/tries;
	printf("Ratio is %f\n", ratio)
	printf("Missed: %d\n",(tries - correct))

endwhile
