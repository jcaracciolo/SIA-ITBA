source('Utils')
source('NeuralFunctions')
% inputSize: Size of each input
% outputSize: Size of the output
% layersSize: Size of each layer
% learningRate: Rate of learning
source('config')

% Trainer functions
% trainerPath: Path to trainer file
%  -- ? getSampleSize()
%  -- ? getSample(int index)
%  -- ? getAnswer(int index)
source(trainerPath)

%  Activation functions
% activationFunctionPath: Path to activate function
%  -- float activate(float x)
%  -- float correction(float x) derivative
source(activationFunctionPath)

% Create array with all sizes
layersSize = [layersSize, outputSize];
global layersCount = length(layersSize)

% Setup RNG
if (exist('randomSeed', 'var') == 1)
    rand('seed',randomSeed)
else
	seed = rand('seed')
	save randState seed
	rand('seed', seed)
endif


%create initial random matrix
global weights = cell(layersCount, 1);
global deltaWeights = cell(layersCount, 1);
global deltas = cell(layersCount,1);

% Initial weights takes into account inputSize
weights(1) = rand(layersSize(1), inputSize+1);
deltaWeights(1) = zeros(layersSize(1), inputSize+1);

for i=2:layersCount
	% Matrix size is layer(i+1) X (layer(i)+1)
	weights(i) = rand(layersSize(i), layersSize(i-1)+1);
	deltaWeights(i) = zeros(layersSize(i), layersSize(i-1)+1);
endfor

%Output vector for each layer
global rawOutputs = cell(layersCount,1);
global outputs = cell(layersCount,1);
global errors = [];
global consecutiveErrorReduction=0;
global epochs = 0;	


while(true)
	epochs +=1;

	samplesShuffled = randperm(getSampleSize());
	for index = samplesShuffled

		% Input must be a column vector with a -1 at the beggining
		input = getSample(index)';
		lastOutput = makeGuess(input);
		expected = getAnswer(index)';

		%Correcting
		% Calculate the last delta to start backpropagation
		deltas{end} = times(correction(rawOutputs{end}), (expected - lastOutput));

		% For each delta, calculate it by correcting the output of the given layer
		% point product by the matrix product of
		% the weigths of the upper layer (removing the threshold column) and
		% the delta of the upper layer. 
		for i=fliplr(1:layersCount-1)
			deltas{i} = times(correction(rawOutputs{i}), (weights{i+1}(:,2:end)'*deltas{i+1}));
		endfor


		deltaWeights{1} = learningRate*(deltas{1}*([-1; input]')) + alphaMomentum*deltaWeights{1};
		weights{1} += deltaWeights{1};
		for(i=2:layersCount)
			deltaWeights{i} = learningRate*(deltas{i}*([-1; outputs{i-1}]')) + alphaMomentum*deltaWeights{i};
			weights{i} += deltaWeights{i};
		endfor
	endfor

	adaptEta()
	
	if(epochs>1000)
		plot(1:length(errors),errors)
		pause(1000000)
	endif

endwhile
