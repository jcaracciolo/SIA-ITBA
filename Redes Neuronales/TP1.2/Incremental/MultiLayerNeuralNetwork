source('Incremental/NeuralFunctions')
% inputSize: Size of each input
% outputSize: Size of the output
% layersSize: Size of each layer
% learningRate: Rate of learning

% Trainer functions
% trainerPath: Path to trainer file
%  -- ? getSampleSize()
%  -- ? getSample(int index)
%  -- ? getAnswer(int index)
source(trainerPath)


% Create array with all sizes
layersSize = [layersSize, outputSize];
global layersCount = length(layersSize);

% Setup RNG
if (exist('randomSeed', 'var') == 1)
    rand('seed',randomSeed)
else
	seed = rand('seed')
	save randState seed
	rand('seed', seed)
endif


%create initial random matrix
global weights = cell(layersCount, 1);
global deltaWeights = cell(layersCount, 1);
global deltas = cell(layersCount,1);

% Initial weights takes into account inputSize
weights(1) = rand(layersSize(1), inputSize+1);
deltaWeights(1) = zeros(layersSize(1), inputSize+1);

for i=2:layersCount
	% Matrix size is layer(i+1) X (layer(i)+1)
	weights(i) = rand(layersSize(i), layersSize(i-1)+1);
	deltaWeights(i) = zeros(layersSize(i), layersSize(i-1)+1);
endfor

%Output vector for each layer
global rawOutputs = cell(layersCount,1);
global outputs = cell(layersCount,1);
global errors = [];
global consecutiveErrorReduction=0;
global epochs = 0;	

sampleSize = getSampleSize();
samples = randperm(sampleSize);
trainingSize = ceil(sampleSize*trainingPercentage);
testingSize = sampleSize - trainingSize;
global trainingIndexes = samples(1:trainingSize);
global testingIndexes = samples(trainingSize+1:end);

maxEpochs = 500;


displayRate=50;
displayCounter=0;


while(epochs<maxEpochs)
	epochs +=1
	displayCounter+=1;

	samplesShuffled = trainingIndexes(randperm(trainingSize));
	for index = samplesShuffled

		% Input must be a column vector with a -1 at the beggining
		input = getSample(index);
		lastOutput = makeGuess(input);
		expected = getAnswer(index);

		%Correcting
		% Calculate the last delta to start backpropagation
		deltas{end} = times(correction(rawOutputs{end}), (expected - lastOutput));

		% For each delta, calculate it by correcting the output of the given layer
		% point product by the matrix product of
		% the weigths of the upper layer (removing the threshold column) and
		% the delta of the upper layer. 
		for i=fliplr(1:layersCount-1)
			deltas{i} = times(correction(rawOutputs{i}), (weights{i+1}(:,2:end)'*deltas{i+1}));
		endfor


		deltaWeights{1} = learningRate*(deltas{1}*([-1; input]')) + alphaMomentum*deltaWeights{1};
		weights{1} += deltaWeights{1};
		for(i=2:layersCount)
			deltaWeights{i} = learningRate*(deltas{i}*([-1; outputs{i-1}]')) + alphaMomentum*deltaWeights{i};
			weights{i} += deltaWeights{i};
		endfor
	endfor

	adaptEta()


	% if(exist('visual', 'var') == 1 && visual)
	% 	trEr=medianTrainingError();
	% 	tEr=medianTestingError();
	% 	scatter(epochs,errors(end),'r','o');
	% 	scatter(epochs, trEr,'b','-');
	% 	scatter(epochs,tEr,'g','*');
	% 	refresh;
	% 	if(displayCounter==displayRate)
	% 		displayCounter=0;
	% 		epochs
	% 		trEr
	% 		tEr
	% 	endif
	% endif

endwhile
% dlmwrite(weightsPath, []);

% for i = 1 : layersCount
% 	csvwrite(weightsPath, weights{i},"-append");
% endfor
if(exist('outputOn', 'var') == 1 && outputOn)
	save(weightsPath, "weights");
endif
testingError = 0;
correct=0;
for test = testingIndexes
	input = getSample(test);
	guess = makeGuess(input);
	expected = getAnswer(test);

	testingError += (guess - expected)**2;
	if(abs(guess - expected)<validateEpsilon)
		correct+=1;
	endif
endfor

first = floor(maxEpochs*0.1);
sumE = sum(errors(end-first+1:end))/first;
testingError = testingError/(2*length(testingIndexes));
printf('%.20f', testingError)
% printf('Guessed %d out of %d. Ratio: %f\n',correct, testingSize,correct/testingSize) 
if (exist('visual', 'var') == 1 && visual)
	plot(1:length(errors),errors)  
	pause(1000)
endif
