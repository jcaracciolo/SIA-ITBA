source('Batch/NeuralFunctionsBatch')
% inputSize: Size of each input
% outputSize: Size of the output
% layersSize: Size of each layer
% learningRate: Rate of learning

% Trainer functions
% trainerPath: Path to trainer file
%  setUpBatch()
%  -- ? getSampleSize()
%  -- ? getSampleMatrix()
%  -- ? getAnswerMatrix()
source(trainerPath)
setUpBatch()

%  Activation functions
% activationFunctionPath: Path to activate function
%  -- float activate(float x)
%  -- float correction(float x) derivative
source(activationFunctionPath)

% Create array with all sizes
layersSize = [layersSize, outputSize];
global layersCount = length(layersSize)

% Setup RNG
if (exist('randomSeed', 'var') == 1)
    rand('seed',randomSeed)
else
	seed = rand('seed')
	save randState seed
	rand('seed', seed)
endif

global sampleSize = getSampleSize()
global samples = randperm(sampleSize);
global trainingSize = ceil(sampleSize*trainingPercentage);
global testingSize = sampleSize - trainingSize;
global trainingIndexes = samples(1:trainingSize);
global testingIndexes = samples(trainingSize+1:end);

global trainingMatrix = getSampleMatrix()(:,trainingIndexes);
global testingMatrix = getSampleMatrix()(:,testingIndexes);
global trainingAnswerMatrix = getAnswerMatrix()(:,trainingIndexes);
global testingAnswerMatrix = getAnswerMatrix()(:,testingIndexes);

%create initial random matrix
global weights = cell(layersCount,1);
global deltaWeights = cell(layersCount,1);

% Initial weights takes into account inputSize
weights(1) = rand(layersSize(1), inputSize+1);
deltaWeights(1) = zeros(layersSize(1), inputSize+1);

for i=2:layersCount
	% Matrix size is layer(i+1) X (layer(i)+1)
	weights(i) = rand(layersSize(i), layersSize(i-1)+1);
	deltaWeights(i) = zeros(layersSize(i), layersSize(i-1)+1);
endfor

global lastWeights = weights;
global lastDeltaWeights = deltaWeights;

%Output vector for each layer
global rawOutputs = cell(layersCount, 1);
global outputs = cell(layersCount, 1);
global deltas = cell(layersCount, 1);

global epochs = 1;	
global errors = [medianBatchError()];
global trainingErrors=[medianTrainingError()];
global testingErrors=[medianTestingError()];
global lastError=errors(end);
global lastTrainingError=trainingErrors(end);
global lastTestingError=testingErrors(end);

global membraines = zeros(1,trainingSize) -1;

displayCounter=0;
while(lastError>errorGoal)
	lastError
	minPlot = 2;
	if(exist('visual', 'var') == 1 && visual)
		if(length(errors) >  minPlot)
			hold on;
			mini = min([minPlot, length(errors)]);
			if(displayCounter==displayRate)
				set(gca,'Color',[0,0,0]) 
	            corrected = getCorrected(); 
	            ratio = corrected/testingSize*100; 
	            title ( 
	              sprintf('{\\fontsize{12} Errors - current LearningRate: %.3f - guessed: %d/%d (%.2f%%)}', 
	              learningRate,corrected,testingSize,ratio)); 
	            % plot(mini:length(errors),errors(mini:end),'w');  
	            plot(mini:length(testingErrors),testingErrors(mini:end),'g');  
	            plot(mini:length(trainingErrors),trainingErrors(mini:end),'r');  
	            legend( 
	              % sprintf('{\\fontsize{12} MedianError %f\n}',lastError), 
	              sprintf('{\\fontsize{12} trainingError %f\n}',lastTrainingError), 
	              sprintf('{\\fontsize{12} testingError %f\n}',lastTestingError) 
	            )      
	            refresh;  
	          displayCounter=0;  
			endif
		else
	         displayCounter=0;
		endif
	endif

	lastWeights = weights;
	lastDeltaWeights = deltaWeights;
	epochs +=1;
	displayCounter+=1;

	lastOutput = makeGuessWithMatrix(trainingMatrix);

	%Correcting
	% Calculate the last delta to start backpropagation
	deltas{end} = times(correction(rawOutputs{end}), (trainingAnswerMatrix - lastOutput));

	% For each delta, calculate it by correcting the output of the given layer
	% point product by the matrix product of
	% the weigths of the upper layer (removing the threshold column) and
	% the delta of the upper layer. 
	for i=fliplr(1:layersCount-1)
		deltas{i} = times(correction(rawOutputs{i}), (weights{i+1}(:,2:end)'*deltas{i+1}));
	endfor


	deltaWeights{1} = learningRate*(deltas{1}*([membraines; trainingMatrix]')) + alphaMomentum*deltaWeights{1};
	weights{1} += deltaWeights{1};
	for(i=2:layersCount)
		deltaWeights{i} = learningRate*(deltas{i}*([membraines; outputs{i-1}]')) + alphaMomentum*deltaWeights{i};
		weights{i} += deltaWeights{i}/trainingSize;
	endfor

	adaptEta()

endwhile

testingError = 0;
correct=0;
guess = makeGuessWithMatrix(testingMatrix);
expected = testingAnswerMatrix;
for i=1:testingSize
	if(abs(guess(i)-expected(i))<validateEpsilon)
		correct+=1;
	endif
endfor

printf('Guessed %d out of %d. Ratio: %f wit a %f error\n',
	correct, testingSize,correct/testingSize, validateEpsilon) 
if (exist('visual', 'var') == 1 && visual)
	plot(1:length(errors),errors)  
	pause()
endif
